{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdba4a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torchaudio\n",
    "import librosa\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5548eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 128\n",
    "num_layers = 3\n",
    "output_dim = 40\n",
    "latent_dim = 16\n",
    "seq_length = len(training_data[0])\n",
    "batch_size = 5\n",
    "learning_rate= 0.01\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3aa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=16000, n_mfcc=40):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_list = os.listdir(root_dir)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mfcc = n_mfcc\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.root_dir, self.file_list[idx])\n",
    "        waveform, _ = librosa.load(file_path, sr=self.sample_rate, mono=True)\n",
    "        mfcc = librosa.feature.mfcc(y=waveform, sr=self.sample_rate, n_mfcc=self.n_mfcc)\n",
    "        mfcc_tensor = torch.tensor(mfcc.T)  # Transpose for PyTorch compatibility (seq_length, n_mfcc)\n",
    "        return mfcc_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de8caa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkdrn\\AppData\\Local\\Temp\\ipykernel_11788\\3050978139.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_data = nn.utils.rnn.pad_sequence([torch.tensor(seq).clone().detach().requires_grad_(True) for seq in training_data], batch_first=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "training_data = MFCCDataset(r\"C:\\Users\\rkdrn\\Untitled Folder\\dataset\")\n",
    "training_data_iterable = [training_data[i] for i in range(len(training_data))]\n",
    "training_data = nn.utils.rnn.pad_sequence([torch.tensor(seq).clone().detach().requires_grad_(True) for seq in training_data], batch_first=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f5ab29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = train_test_split(training_data, test_size=10)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader= DataLoader(validation_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d5a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del training_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3bd4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, nb_feature, dropout=0, device=torch.device('cpu')):\n",
    "        super(LSTMAutoEncoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(num_layers, hidden_size, nb_feature, dropout, device)\n",
    "        self.decoder = Decoder(num_layers, hidden_size, nb_feature, dropout, device)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        output = torch.zeros(size=input_seq.shape, dtype=torch.float)\n",
    "        hidden_cell = self.encoder(input_seq)\n",
    "        input_decoder = input_seq[:, -1, :].view(input_seq.shape[0], 1, input_seq.shape[2])\n",
    "        for i in range(input_seq.shape[1] - 1, -1, -1):\n",
    "            output_decoder, hidden_cell = self.decoder(input_decoder, hidden_cell)\n",
    "            input_decoder = output_decoder\n",
    "            output[:, i, :] = output_decoder[:, 0, :]\n",
    "        return output\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, nb_feature, dropout=0, device=torch.device('cpu')):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = nb_feature\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=nb_feature, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout, bias=True)\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        self.hidden_cell = (\n",
    "            torch.randn((self.num_layers, batch_size, self.hidden_size), dtype=torch.float).to(self.device),\n",
    "            torch.randn((self.num_layers, batch_size, self.hidden_size), dtype=torch.float).to(self.device)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        self.initHidden(input_seq.shape[0])\n",
    "        _, self.hidden_cell = self.lstm(input_seq, self.hidden_cell)\n",
    "        return self.hidden_cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, nb_feature, dropout=0, device=torch.device('cpu')):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.input_size = nb_feature\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=nb_feature, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout, bias=True)\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=nb_feature)\n",
    "\n",
    "    def forward(self, input_seq, hidden_cell):\n",
    "        output, hidden_cell = self.lstm(input_seq, hidden_cell)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519757f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, x,lengths):\n",
    "        packed_x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return hidden[-1]\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, seq_length):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out).reshape(-1, seq_length, output_dim)\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, seq_length):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = LSTMDecoder(hidden_dim, hidden_dim, num_layers, output_dim)\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def forward(self, x):\n",
    "        context = self.encoder(x)\n",
    "        # Repeat the context vector to feed it to each time step of the decoder\n",
    "        repeat_context = context.unsqueeze(1).repeat(1, self.seq_length, 1)\n",
    "        output = self.decoder(repeat_context, self.seq_length)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ac9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Encoder LSTM\n",
    "        self.lstm_encoder = nn.LSTM(input_dim, hidden_dim, 3, batch_first=True)\n",
    "        # Decoder LSTM\n",
    "        self.lstm_decoder = nn.LSTM(hidden_dim, input_dim, 3, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        _, (hidden, _) = self.lstm_encoder(x)\n",
    "        \n",
    "        # Replicate the hidden state across time steps\n",
    "        repeated_hidden = hidden.repeat(1, x.size(1), 1)\n",
    "        \n",
    "        # Decoder\n",
    "        output, _ = self.lstm_decoder(repeated_hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c473e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 128\n",
    "num_layers = 3\n",
    "output_dim = 40\n",
    "seq_length = len(training_data[0])\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05654988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = RecurrentAutoencoder(seq_length, input_dim,  16)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897c0944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LSTMAutoEncoder(3, hidden_dim, output_dim)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78814260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch, data in tqdm(enumerate(dataloader, 1), total=len(dataloader)):\n",
    "        inputs = data\n",
    "\n",
    "        # 순방향 전달 및 손실 계산\n",
    "        recon = model(inputs)\n",
    "        loss = loss_fn(recon, inputs)\n",
    "\n",
    "        # 손실값을 역전파하여 모델을 갱신합니다.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 통계 출력\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for data in dataloader:\n",
    "            inputs = data  \n",
    "            recon = model(inputs)\n",
    "            test_loss += loss_fn(recon, inputs).item()\n",
    "    \n",
    "    epoch_loss = test_loss / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fbc47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▍                                                                           | 1/18 [07:19<2:04:25, 439.12s/it]"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_loss = test(train_dataloader, model, loss_fn)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f\"Train Error: {train_loss:.8f}\")\n",
    "\n",
    "    val_loss = test(validation_dataloader, model, loss_fn)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(f\"Validation Error: {val_loss:.8f}\")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a57221",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "n_features = 1\n",
    "latent_dim = 10\n",
    "\n",
    "ENCODER_1 = 512\n",
    "ENCODER_2 = 256\n",
    "ENCODER_3 = 128\n",
    "\n",
    "LATENT_VECTOR = 64\n",
    "\n",
    "DECODER_1 = 512\n",
    "DECODER_2 = 256\n",
    "DECODER_3 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, ENCODER_1),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(ENCODER_1, ENCODER_2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(ENCODER_2, ENCODER_3),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(ENCODER_3, LATENT_VECTOR),\n",
    "        )\n",
    "        self.initialize_weights(self.encoder)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(LATENT_VECTOR, DECODER_1),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(DECODER_1, DECODER_2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(DECODER_2, DECODER_3),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(DECODER_3, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.initialize_weights(self.decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def initialize_weights(self, module):\n",
    "        for m in module:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263797fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe0481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
